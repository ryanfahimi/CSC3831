{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSC3831 Final Assessment - Part I: Data Engineering\n"
      ],
      "metadata": {
        "id": "fkPM_6BCj0C4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "297NrWRXjtYD"
      },
      "outputs": [],
      "source": [
        "# Loading in standard packages for analysis, feel free to add an extra packages you'd like to use here\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import missingno as msno\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import KNNImputer, IterativeImputer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score, root_mean_squared_error\n",
        "\n",
        "# Loading in the corrupted dataset to be used in analysis and imputation\n",
        "houses_corrupted = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/CORRUPTED/HOUSES/houses_0.1_MAR.csv', header=0)\n",
        "# Remove an artifact from the dataset\n",
        "houses_corrupted.drop([\"Unnamed: 0\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we've loaded in a corrupted version of a housing dataset. The anomalies need to be dealt with and missing values imputed."
      ],
      "metadata": {
        "id": "_LL4clGtj2Fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Understanding [7]\n",
        "- Perform ad hoc EDA to understand and describe what you see in the raw dataset\n",
        "  - Include graphs, statistics, and written descritpions as appropriate\n",
        "  - Any extra information about the data you can provide here is useful, think about performing an analysis (ED**A**), what would you find interesting or useful?\n",
        "- Identify features with missing records, outlier records\n"
      ],
      "metadata": {
        "id": "sqqcUVEZj3t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial data overview\n",
        "print(\"Dataset Shape:\", houses_corrupted.shape)\n",
        "print(\"\\nFeature Types:\")\n",
        "print(houses_corrupted.dtypes)\n",
        "\n",
        "# Basic statistical summary\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(houses_corrupted.describe())"
      ],
      "metadata": {
        "id": "Vu8I1Bncj6S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:** I printed the size of the dataset, its types of data, and basic statistics for each feature.\n",
        "\n",
        "**Reasoning:**\n",
        "- Provided an overview of the dataset's size and structure by checking its shape\n",
        "- Examined data types to understand and categorize the different features\n",
        "- Used statistical summaries to identify which features need attention, through their center, spread, and missing values [1], [2]\n",
        "\n",
        "**Results Analysis:**\n",
        "* There are 20,640 rows, with 9 columns each.\n",
        "* The features can be divided into three categories:\n",
        "  1. House Features\n",
        "    * median_house_value\n",
        "    * housing_median_age\n",
        "    * total_rooms\n",
        "    * total_bedrooms\n",
        "  2. Demographic Features\n",
        "    * median_income\n",
        "    * population\n",
        "    * households\n",
        "  3. Geographic Features\n",
        "    * latitude\n",
        "    * longitude\n",
        "* Three columns have missing data: median_income, housing_median_age, and population\n",
        "\n"
      ],
      "metadata": {
        "id": "EIwcl3QHj7Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution analysis with skewness\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, col in enumerate(houses_corrupted.columns):\n",
        "    sns.histplot(data=houses_corrupted, x=col, ax=axes[i], kde=True)\n",
        "    skewness = houses_corrupted[col].skew()\n",
        "    axes[i].set_title(f'{col}\\nSkewness: {skewness:.2f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare classical vs robust statistics for skewed variables\n",
        "print(\"\\nComparison of Classical vs Robust Statistics:\")\n",
        "non_normal_cols = ['median_income', 'total_rooms', 'total_bedrooms',\n",
        "               'population', 'households', 'latitude', 'longitude']\n",
        "\n",
        "for col in non_normal_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"Classical measures:\")\n",
        "    print(f\"Mean: {houses_corrupted[col].mean():.2f}\")\n",
        "    print(f\"Standard Deviation: {houses_corrupted[col].std():.2f}\")\n",
        "    print(f\"Robust measures:\")\n",
        "    print(f\"Median: {houses_corrupted[col].median():.2f}\")\n",
        "    print(f\"MAD: {1.4826 * abs(houses_corrupted[col] - houses_corrupted[col].median()).median():.2f}\")"
      ],
      "metadata": {
        "id": "bFU3a1l8j-In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "I created histogram plots for each feature to see their distributions and calculated skewness values. I also compared classic statistics (mean, standard deviation) with robust statistics (median, MAD) for features that weren't normally distributed.\n",
        "\n",
        "**Reasoning:**\n",
        "- Plotted distributions and calculated skewness to further understand feature distributions [2]\n",
        "- Compared classical and robust statistics to see which will be most reliable for each feature [3]\n",
        "\n",
        "**Results Analysis:**\n",
        "\n",
        "- Non-normal features can be grouped into three types:\n",
        "  - Very skewed features (skewness > 3)\n",
        "    * total_rooms, total_bedrooms, population, and households\n",
        "  - Somewhat skewed features (skewness 0.5-3)\n",
        "    * median_house_value and median_income are less skewed but still not normal\n",
        "  - Bimodal features\n",
        "    * latitude and longitude show two distinct groups, likely showing different city clusters\n",
        "\n",
        "- There are some extreme high values pulling the means and standard deviations up\n",
        "  * The means are always higher than the medians\n",
        "  * Standard deviations are much larger than MAD values\n"
      ],
      "metadata": {
        "id": "DPEv7Va7j-ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iqr_outliers(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return (data < lower_bound) | (data > upper_bound)\n",
        "\n",
        "# Box plots for initial outlier visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(houses_corrupted.columns):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    sns.boxplot(x=houses_corrupted[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# IQR-based outlier detection\n",
        "print(\"IQR-based Outlier Analysis:\")\n",
        "for col in houses_corrupted.columns:\n",
        "    outliers = houses_corrupted[calculate_iqr_outliers(houses_corrupted[col])]\n",
        "\n",
        "    # Only print results if meaningful number of outliers found (>1%)\n",
        "    outlier_percentage = (len(outliers)/len(houses_corrupted[col].dropna()))*100\n",
        "    if outlier_percentage > 1:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"Number of outliers: {len(outliers)}\")\n",
        "        print(f\"Percentage of outliers: {outlier_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "obXHsQipkCGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I created box plots for each feature to visually spot outliers and then used the IQR method to find outliers statistically. I focused on features where outliers made up more than 1% of the data.\n",
        "\n",
        "**Reasoning:**\n",
        "- The IQR method is good for a first check because of its simplicity [1]\n",
        "- Setting a 1% threshold focuses on features where outliers might actually be a problem\n",
        "\n",
        "**Results Analysis:**\n",
        "\n",
        "- Size features had the most outliers\n",
        "  * total_rooms: 6.24%\n",
        "  * total_bedrooms: 6.21%\n",
        "  * households: 5.91%\n",
        "  * population: 5.80%\n",
        "- Value features also had outliers\n",
        "  * median_house_value: 5.19%\n",
        "  * median_income: 3.03%\n",
        "- Most features have outliers making up more than 5% of their data"
      ],
      "metadata": {
        "id": "ki4TMU9ykCo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different transformations on skewed columns\n",
        "skewed_cols = ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
        "print(\"Skewness Comparison Across Different Transformations:\")\n",
        "\n",
        "# Compare transformations for each column\n",
        "transformed_data = houses_corrupted.copy()\n",
        "for col in skewed_cols:\n",
        "    # Calculate skewness for original and transformed data\n",
        "    original_skew = houses_corrupted[col].skew()\n",
        "    log_skew = np.log(houses_corrupted[col]).skew()\n",
        "    sqrt_skew = np.sqrt(houses_corrupted[col]).skew()\n",
        "    cbrt_skew = np.cbrt(houses_corrupted[col]).skew()\n",
        "\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"Original skewness: {original_skew:.3f}\")\n",
        "    print(f\"Log transform skewness: {log_skew:.3f}\")\n",
        "    print(f\"Square root transform skewness: {sqrt_skew:.3f}\")\n",
        "    print(f\"Cube root transform skewness: {cbrt_skew:.3f}\")\n",
        "\n",
        "    # Identify best transformation\n",
        "    skews = {'log': abs(log_skew), 'sqrt': abs(sqrt_skew), 'cbrt': abs(cbrt_skew)}\n",
        "    best_transform = min(skews, key=skews.get)\n",
        "    print(f\"Best transformation: {best_transform}\")\n",
        "\n",
        "    # Apply cube root transformation\n",
        "    transformed_data[col] = np.cbrt(houses_corrupted[col])\n",
        "\n",
        "# Visualize original vs transformed distributions\n",
        "fig, axes = plt.subplots(len(skewed_cols), 2, figsize=(15, 4*len(skewed_cols)))\n",
        "for i, col in enumerate(skewed_cols):\n",
        "    # Original distribution\n",
        "    sns.histplot(data=houses_corrupted, x=col, ax=axes[i, 0], kde=True)\n",
        "    axes[i, 0].set_title(f'Original {col}\\nSkewness: {houses_corrupted[col].skew():.3f}')\n",
        "\n",
        "    # Transformed distribution\n",
        "    sns.histplot(data=transformed_data[col], ax=axes[i, 1], kde=True)\n",
        "    axes[i, 1].set_title(f'Cube Root Transformed {col}\\nSkewness: {transformed_data[col].skew():.3f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qi4RiO_NkFzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I tested three different ways to transform the skewed data:\n",
        "- Log transformation\n",
        "- Square root transformation\n",
        "- Cube root transformation\n",
        "\n",
        "Then I compared how well each method worked by looking at the skewness before and after. I chose the transformation that performed best on average and applied it to the entire dataset.\n",
        "\n",
        "Finally, I made plots to see the differences between the best transformed distributions and the original distributions visually.\n",
        "\n",
        "**Reasoning:**\n",
        "- Finding correlations between features is easier when they have normal distributions [1]\n",
        "- Using the same transformation for most features maintains consistency\n",
        "- I didn't transform latitude and longitude because transformations had little effect on their bimodality\n",
        "\n",
        "**Results Analysis:**\n",
        "- median_income\n",
        "  * Started with skewness of 1.588\n",
        "  * Log transformation worked best, reducing skewness to 0.210\n",
        "- All other skewed features\n",
        "  * Started with high skewness (3.41 - 4.83)\n",
        "  * Cube root transformation worked best\n",
        "- The histograms show the data looks much more normal after transformation\n",
        "\n"
      ],
      "metadata": {
        "id": "naHgXFh1kHcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Analysis\n",
        "# Original correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = houses_corrupted.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title(\"Original Feature Correlations\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Transformed correlations for skewed variables\n",
        "plt.figure(figsize=(10, 8))\n",
        "transformed_correlation = transformed_data[skewed_cols].corr()\n",
        "sns.heatmap(transformed_correlation, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title(\"Correlations of Transformed Features\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "size_cols = ['total_rooms', 'total_bedrooms', 'households', 'population']\n",
        "geo_cols = ['latitude', 'longitude']\n",
        "value_cols = ['median_house_value', 'median_income']\n",
        "\n",
        "# Feature Group Correlations\n",
        "feature_groups = [\n",
        "    (size_cols, \"Size-related Features\"),\n",
        "    (geo_cols, \"Geographic Features\"),\n",
        "    (value_cols, \"Value-related Features\")\n",
        "]\n",
        "\n",
        "for cols, title in feature_groups:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(houses_corrupted[cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Scatter plots for key relationships\n",
        "important_pairs = [\n",
        "    ('total_rooms', 'total_bedrooms'),\n",
        "    ('latitude', 'longitude'),\n",
        "    ('median_house_value', 'median_income')\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, (feat1, feat2) in enumerate(important_pairs):\n",
        "    sns.scatterplot(data=houses_corrupted, x=feat1, y=feat2, ax=axes[i], alpha=0.5)\n",
        "    corr = correlation_matrix.loc[feat1, feat2]\n",
        "    axes[i].set_title(f'{feat1} vs {feat2}\\nCorrelation: {corr:.3f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wivb4UemkJEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I created several correlation analyses using heatmaps and scatter plots:\n",
        "- A correlation heatmap for all features\n",
        "- A heatmap just for the transformed features\n",
        "- Separate heatmaps for related feature groups (size, location, value)\n",
        "- Scatter plots for some key relationships\n",
        "\n",
        "**Reasoning:**\n",
        "- Created correlation heatmaps to understand relationships between features [1], [2], [3]\n",
        "- Grouped transormed features to demonstrate the effect of transforming the data on correlation\n",
        "- Grouped related features together to identify strong group relationships\n",
        "- Used scatter plots to visualize the strongest correlations and verify their patterns [1], [2]\n",
        "\n",
        "**Results Analysis:**\n",
        "- Size features are strongly related\n",
        "  * total_rooms and total_bedrooms are the most highly correlated (0.98)\n",
        "- Geographic features\n",
        "  * latitude and longitude have a strong negative correlation (-0.92)\n",
        "- Value features\n",
        "  * median_income and median_house_value are moderately correlated (0.69)\n",
        "\n"
      ],
      "metadata": {
        "id": "57XHKGl3kKcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Value Analysis\n",
        "\n",
        "# Basic missing value summary\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing Count': houses_corrupted.isnull().sum(),\n",
        "    'Missing Percentage': (houses_corrupted.isnull().sum() / len(houses_corrupted)) * 100\n",
        "})\n",
        "print(\"Missing Value Summary:\")\n",
        "print(missing_summary)\n",
        "\n",
        "# Visualize missing patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "msno.matrix(houses_corrupted)\n",
        "plt.title(\"Missing Value Pattern\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze overlap between features with missing values\n",
        "missing_cols = ['median_income', 'housing_median_age', 'population']\n",
        "print(\"\\nMissing Value Overlap Analysis:\")\n",
        "for i in range(len(missing_cols)):\n",
        "    for j in range(i+1, len(missing_cols)):\n",
        "        col1, col2 = missing_cols[i], missing_cols[j]\n",
        "        both_missing = houses_corrupted[houses_corrupted[col1].isnull() &\n",
        "                                      houses_corrupted[col2].isnull()].shape[0]\n",
        "        print(f\"{col1} and {col2} overlap: {both_missing} rows ({(both_missing/2064)*100:.2f}%)\")\n",
        "\n",
        "# Compare medians between missing and present values\n",
        "complete_cols = ['median_house_value', 'households', 'total_rooms', 'total_bedrooms', 'latitude', 'longitude']\n",
        "print(\"\\nSignificant Patterns in Missingness (>10% difference):\")\n",
        "\n",
        "for missing_col in missing_cols:\n",
        "    missing_mask = houses_corrupted[missing_col].isnull()\n",
        "\n",
        "    for complete_col in complete_cols:\n",
        "        missing_median = houses_corrupted[missing_mask][complete_col].median()\n",
        "        present_median = houses_corrupted[~missing_mask][complete_col].median()\n",
        "        diff_percent = ((missing_median - present_median) / present_median) * 100\n",
        "\n",
        "        if abs(diff_percent) > 10:\n",
        "            print(f\"\\n{complete_col} when {missing_col} is missing vs present:\")\n",
        "            print(f\"Missing: {missing_median:.2f}, Present: {present_median:.2f}\")\n",
        "            print(f\"Difference: {diff_percent:.2f}%\")"
      ],
      "metadata": {
        "id": "4rk8fTMskL70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "- Calculated basic missing value stats\n",
        "- Created a visual pattern of missing values using missingno\n",
        "- Checked how missing values overlap between features\n",
        "- Compared values in rows with and without missing data\n",
        "\n",
        "**Reasoning:**\n",
        "- Generated summary statistics to measure the scale of the missing data\n",
        "- Visualized missing patterns to check for any consistent gaps in the data [3]\n",
        "- Analyzed missing value overlap to determine if missingness is related between incomplete records\n",
        "- Compared statistics between complete and incomplete records to classify the missing data type (MCAR, MAR, or MNAR) [4]\n",
        "\n",
        "**Results Analysis:**\n",
        "- Three features have exactly 10% missing values:\n",
        "  * median_income\n",
        "  * housing_median_age\n",
        "  * population\n",
        "- The missing values don't have much overlap (9-12% overlap)\n",
        "  - The low overlap in missing values suggests missing values occur independently from one another\n",
        "- When rows with and without missing values are compared, patterns emerge\n",
        "  * Removing missing median_income rows results in higher median house values\n",
        "  * Removing missing housing_median_age rows results in larger houses\n",
        "  * Removing missing population rows results in much larger houses and higher values\n",
        "- The relationship betwen missing values and other features suggests this is Missing at Random (MAR) data\n"
      ],
      "metadata": {
        "id": "GwEaDlv2kMgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Outlier Identification [10]\n",
        "- Utilise a statistical outlier detection approach (i.e., **no** KNN, LOF, 1Class SVM)\n",
        "- Utilise an algorithmic outlier detection method of your choice\n",
        "- Compare results and decide what to do with identified outliers\n",
        "  - Include graphs, statistics, and written descriptions as appropriate\n",
        "- Explain what you are doing, and why your analysis is appropriate\n",
        "- Comment on benefits/detriments of statistical and algorithmic outlier detection approaches\n"
      ],
      "metadata": {
        "id": "9NbOGKU0kPtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Outlier Detection\n",
        "def calculate_robust_zscore_outliers(data, threshold=3):\n",
        "    median = data.median()\n",
        "    mad = np.median(np.abs(data - median)) * 1.483\n",
        "    rob_z_scores = (data - median) / mad\n",
        "    return abs(rob_z_scores) > threshold\n",
        "\n",
        "data = {}\n",
        "# Store robust z-score results\n",
        "robust_results = {}\n",
        "# Detect outliers for all numeric columns\n",
        "print(\"Outlier Detection Results:\\n\")\n",
        "for col in houses_corrupted.columns:\n",
        "    col_data = houses_corrupted[col].dropna()\n",
        "    data[col] = col_data\n",
        "\n",
        "    # Calculate outliers using both methods\n",
        "    robust_outliers = calculate_robust_zscore_outliers(col_data)\n",
        "    iqr_outliers = calculate_iqr_outliers(col_data)\n",
        "\n",
        "    # Calculate percentages\n",
        "    robust_pct = (robust_outliers.sum() / len(col_data)) * 100\n",
        "    robust_results[col] = {\n",
        "            'outliers': robust_outliers,\n",
        "            'percentage': robust_pct,\n",
        "        }\n",
        "    iqr_pct = (iqr_outliers.sum() / len(col_data)) * 100\n",
        "\n",
        "    # Only show if significant outliers found (>1%)\n",
        "    if robust_pct > 1 or iqr_pct > 1:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"Robust Z-score outliers: {robust_pct:.2f}%\")\n",
        "        print(f\"IQR outliers: {iqr_pct:.2f}%\")\n",
        "\n",
        "        # Visualize outliers\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.scatter(range(len(col_data)), col_data, c='lightgray', s=10, label='Normal')\n",
        "        plt.scatter(np.where(robust_outliers)[0], col_data[robust_outliers],\n",
        "                   c='red', s=10, alpha=0.4, label='Robust Z-score')\n",
        "        plt.scatter(np.where(iqr_outliers)[0], col_data[iqr_outliers],\n",
        "                   c='blue', s=10, alpha=0.2, label='IQR')\n",
        "        plt.title(f'Outlier Detection: {col}')\n",
        "        plt.ylabel('Value')\n",
        "        plt.xlabel('Index')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "eNoiOp5AkRc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I used two statistical methods to find outliers:\n",
        "- Robust z-score method using median and MAD\n",
        "- IQR method using 1.5 times the interquartile range\n",
        "\n",
        "I calculated the percentage of outliers for each method and made plots showing where these outliers appear.\n",
        "\n",
        "**Reasoning:**\n",
        "- Used robust z-score to handle skewed distributions since it uses median/MAD instead of mean/standard deviation [5]\n",
        "- Used IQR method as a comparison point since it's simple [1]\n",
        "- Visualized results to intuitively check outliers and compare method agreement [6]\n",
        "- Focused only on significant outlier percentages (>1%) to identify notable features\n",
        "\n",
        "\n",
        "**Results Analysis:**\n",
        "- Size features had the most outliers (~6%)\n",
        "- Value features had fewer outliers\n",
        "  * median_house_value: ~5%\n",
        "  * median_income: ~3%\n",
        "- Geographic features had the least outliers\n",
        "  * Only latitude showed significant outliers at 2.5% with the robust z-score method\n",
        "- The plots show both methods usually agreed on what was an outlier\n",
        "- The robust z-score method is the most appropriate choice because it uses median and MAD, which works better with the skewed features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8v0Mn5twkSDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store KNN results\n",
        "knn_results = {}\n",
        "print(\"Algorithmic Outlier Detection Results:\\n\")\n",
        "for col in houses_corrupted:\n",
        "    # Prepare column data\n",
        "    col_data = data[col].values.reshape(-1, 1)\n",
        "\n",
        "    # KNN Global Detection\n",
        "    nbrs = NearestNeighbors(n_neighbors=30).fit(col_data)\n",
        "    distances, _ = nbrs.kneighbors(col_data)\n",
        "    avg_distances = distances.mean(axis=1)\n",
        "    threshold = np.percentile(avg_distances, 95)\n",
        "    knn_outliers = avg_distances > threshold\n",
        "    knn_pct = (knn_outliers.sum() / len(col_data)) * 100\n",
        "\n",
        "    if knn_pct > 1:\n",
        "      knn_results[col] = {\n",
        "          'outliers': knn_outliers,\n",
        "          'percentage': knn_pct,\n",
        "      }\n",
        "\n",
        "    # Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.025)\n",
        "    lof_outliers = lof.fit_predict(col_data) == -1\n",
        "    lof_pct = (lof_outliers.sum() / len(col_data)) * 100\n",
        "\n",
        "    # Isolation Forest\n",
        "    iso_f = IsolationForest(random_state=42)\n",
        "    iso_outliers = iso_f.fit_predict(col_data) == -1\n",
        "    iso_pct = (iso_outliers.sum() / len(col_data)) * 100\n",
        "\n",
        "    # One-Class SVM\n",
        "    ocsvm = OneClassSVM(nu=0.03)\n",
        "    svm_outliers = ocsvm.fit_predict(col_data) == -1\n",
        "    svm_pct = (svm_outliers.sum() / len(col_data)) * 100\n",
        "\n",
        "    # Only show results if significant outliers found\n",
        "    if max(knn_pct, lof_pct, iso_pct, svm_pct) > 1:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"KNN Outliers: {knn_pct:.2f}%\")\n",
        "        print(f\"LOF Outliers: {lof_pct:.2f}%\")\n",
        "        print(f\"Isolation Forest Outliers: {iso_pct:.2f}%\")\n",
        "        print(f\"One-Class SVM Outliers: {svm_pct:.2f}%\")\n",
        "\n",
        "        # Visualize results\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        methods = ['KNN', 'LOF', 'Isolation Forest', 'SVM']\n",
        "        outliers = [knn_outliers, lof_outliers, iso_outliers, svm_outliers]\n",
        "        colors = ['red', 'purple', 'orange', 'green']\n",
        "\n",
        "        for i, (method, outlier_mask, color) in enumerate(zip(methods, outliers, colors)):\n",
        "            axes[i].scatter(range(len(col_data)), col_data, c='lightgray', s=10, label='Normal')\n",
        "            axes[i].scatter(np.where(outlier_mask)[0], col_data[outlier_mask],\n",
        "                          c=color, s=10, label='Outliers')\n",
        "            axes[i].set_title(f'{method} Outliers: {col}')\n",
        "            axes[i].set_ylabel('Value')\n",
        "            axes[i].set_xlabel('Index')\n",
        "            axes[i].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "_0R8q8AdkUzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I used four different algorithmic methods to find outliers:\n",
        "- KNN Anomaly Detection\n",
        "- Local Outlier Factor\n",
        "- Isolation Forest\n",
        "- One-Class SVM\n",
        "\n",
        "I calculated the percentage of outliers for each method and made plots showing where these outliers appear.\n",
        "\n",
        "**Reasoning:**\n",
        "- Implemented multiple algorithms to compare their performance on different feature types [5]\n",
        "- Used KNN and LOF to identify global and local outliers respectively [5] [8]\n",
        "- Used Isolation Forest for cutting-edge tree based method [5]\n",
        "- Used One-Class SVM to test a boundary-based method [5], [8]\n",
        "- Visualized results to intuitively check outliers and compare method agreement [6]\n",
        "\n",
        "\n",
        "**Results Analysis:**\n",
        "- KNN was most consistent\n",
        "  * Found ~5% outliers in most features\n",
        "  * Showed clear separation between normal and outlier points\n",
        "- LOF was more conservative\n",
        "  * Found 2-2.5% outliers\n",
        "  * Sometimes marked points in dense areas as outliers\n",
        "- Isolation Forest found too many outliers\n",
        "  * Found 17-63% outliers\n",
        "  * Seemed especially sensitive with normally distributed features\n",
        "- One-Class SVM was inconsistent\n",
        "  * Found between 2-13% outliers\n",
        "  * Showed strange strip patterns in its detections\n",
        "- KNN anomoly detection is the best performing algorithmic method\n",
        "  - KNN works best with global anomolies [5]\n",
        "  - It avoids the density issues seen with LOF\n",
        "  - It's more reliable than Isolation Forest, which found too many outliers\n",
        "  - It's simpler to understand and tune than One-Class SVM"
      ],
      "metadata": {
        "id": "hzfyu2EXkWHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare k=20 configuration with stored k=30 results\n",
        "print(\"KNN Parameter Comparison Results:\\n\")\n",
        "for col in knn_results:\n",
        "   col_data = data[col].values.reshape(-1, 1)\n",
        "\n",
        "   # Test k=20 configuration\n",
        "   nbrs = NearestNeighbors(n_neighbors=20).fit(col_data)\n",
        "   distances, _ = nbrs.kneighbors(col_data)\n",
        "   avg_distances = distances.mean(axis=1)\n",
        "   threshold = np.percentile(avg_distances, 95)\n",
        "   k20_outliers = avg_distances > threshold\n",
        "   k20_pct = (k20_outliers.sum() / len(col_data)) * 100\n",
        "\n",
        "   print(f\"\\n{col}:\")\n",
        "   print(f\"k=20: {k20_pct:.2f}%\")\n",
        "   print(f\"k=30: {knn_results[col]['percentage']:.2f}%\")\n",
        "\n",
        "   # Visualize outliers\n",
        "   plt.figure(figsize=(10, 4))\n",
        "   plt.scatter(range(len(col_data)), col_data, c='lightgray', s=10, label='Normal')\n",
        "   plt.scatter(np.where(k20_outliers)[0], col_data[k20_outliers],\n",
        "              c='red', s=10, alpha=0.4, label='k=20')\n",
        "   plt.scatter(np.where(knn_results[col]['outliers'])[0], col_data[knn_results[col]['outliers']],\n",
        "              c='blue', s=10, alpha=0.2, label='k=30')\n",
        "   plt.title(f'Outlier Detection: {col}')\n",
        "   plt.ylabel('Value')\n",
        "   plt.xlabel('Index')\n",
        "   plt.legend()\n",
        "   plt.show()"
      ],
      "metadata": {
        "id": "7W4y6-CekXlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I compared two different hyperparameters for KNN outlier detection:\n",
        "- Using 20 nearest neighbors (k=20)\n",
        "- Using 30 nearest neighbors (k=30)\n",
        "\n",
        "I calculated the percentage of outliers for each method and made plots showing where these outliers appear.\n",
        "\n",
        "\n",
        "**Reasoning:**\n",
        "- Tested different k values to find the optimal neighborhood size [5]\n",
        "- Used k=20 as a lower bound to avoid false positives [5]\n",
        "- Used k=30 as an upper bound to avoid false negatives [5]\n",
        "- Visualized results to intuitively check outliers and compare method agreement [6]\n",
        "\n",
        "**Results Analysis:**\n",
        "- Both settings found similar percentages of outliers\n",
        "- k=30 is the best performing final hyperparameter\n",
        "  - k=20 cons:\n",
        "    * Sometimes marked points in dense areas as outliers\n",
        "    * Missed some obvious outliers in the tails\n",
        "    * Was too sensitive to lower-end outliers\n",
        "  - k=30 pros:\n",
        "    * Clearer separation between normal and outlier points\n",
        "    * More consistent in finding tail outliers"
      ],
      "metadata": {
        "id": "uGbTfi5_kZJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare KNN outliers with stored robust z-score outliers\n",
        "print(\"Algorithmic vs Statistical Method Comparison:\\n\")\n",
        "for col in knn_results:\n",
        "    col_data = data[col]\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"KNN Outliers: {knn_results[col]['percentage']:.2f}%\")\n",
        "    print(f\"Robust Z-score Outliers: {robust_results[col]['percentage']:.2f}%\")\n",
        "\n",
        "    # Visualize outliers\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.scatter(range(len(col_data)), col_data, c='lightgray', s=10, label='Normal')\n",
        "    plt.scatter(np.where(knn_results[col]['outliers'])[0], col_data[knn_results[col]['outliers']],\n",
        "                c='red', s=10, alpha=0.4, label='KNN')\n",
        "    plt.scatter(np.where(robust_results[col]['outliers'])[0], col_data[robust_results[col]['outliers']],\n",
        "                c='blue', s=10, alpha=0.2, label='Robust Z-score')\n",
        "    plt.title(f'Outlier Detection: {col}')\n",
        "    plt.ylabel('Value')\n",
        "    plt.xlabel('Index')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-2x0WrHkkayX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I compared the best statistical method (robust z-score) with the best algorithmic method (KNN with k=30)\n",
        "- Calculated outlier percentages for both methods\n",
        "- Made plots showing where these outliers appear\n",
        "\n",
        "**Reasoning:**\n",
        "* Compared best-performing statistical and algorithmic methods to validate outlier detection\n",
        "- Visualized results to intuitively check outliers and compare method agreement [6]\n",
        "\n",
        "**Results Analysis:**\n",
        "\n",
        "- Normal features:\n",
        "  * Robust z-score found fewer outliers (2.66%)\n",
        "  * KNN consistently found about 5%\n",
        "- Skewed features:\n",
        "  * Robust z-score found more outliers (5.85-6.42%)\n",
        "  * KNN consistently found about 5%\n",
        "- Geographic features:\n",
        "  * Robust z-score was more conservative (2.52%)\n",
        "  * KNN consistently found about 5%\n",
        "- Robust z-score is the most appropriate method for this dataset\n",
        "  * It adapts better to each feature's distribution instead of forcing a fixed percentage like KNN\n",
        "  * It's more conservative with normal and bimodal data, which makes sense because these distributions naturally have fewer true outliers\n",
        "  * It's more aggressive with highly skewed features like room counts, where more extreme values are expected\n",
        "\n"
      ],
      "metadata": {
        "id": "1CDxKywJkcEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing both statistical and algorithmic outlier detection methods, I will determine the most appropriate treatment for the identified outliers.\n",
        "\n",
        "Looking at the characteristics of outliers across different feature types, the outliers show clear patterns:\n",
        "- Size Features\n",
        "  * Mostly big properties like apartment complexes\n",
        "  * When rooms are high, bedrooms and population are also high\n",
        "- Value Features\n",
        "  * Represent luxury homes\n",
        "  * Match with high-income areas\n",
        "- Geographic Features\n",
        "  * Outliers identify properties in unique locations\n",
        "\n",
        "\n",
        "Based on these findings I decided to keep all outliers in the dataset\n",
        "- They represent contextual anomolies that are unusual but valid - like luxury homes or apartment complexes [5]\n",
        "- The relationships between features make sense (e.g., more rooms = more bedrooms)\n",
        "- Removing them would hide important parts of the housing market we're trying to analyze"
      ],
      "metadata": {
        "id": "FeDThTvKkdxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Imputation [10]\n",
        "- Identify which features should be imputed and which should be removed\n",
        "  - Provide a written rationale for this decision\n",
        "- Impute the missing records using KNN imputation\n",
        "- Impute the missing records using MICE imputation\n",
        "- Compare both imputed datasets feature distributions against each other and the non-imputed data\n",
        "- Build a regressor on all three datasets\n",
        "  - Use regression models to predict house median price\n",
        "  - Compare regressors of non-imputed data against imputed datas\n",
        "  - **Note**: If you're struggling to compare against the original dataset focus on comparing the two imputed datasets against each other\n"
      ],
      "metadata": {
        "id": "s1McA12Kkfp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this dataset for comparison against the imputed datasets\n",
        "houses = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/ORIGINAL/houses.csv', header=0)"
      ],
      "metadata": {
        "id": "DyKsHGJ7kh3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to my missing values analysis:\n",
        "- Three features each have 10% missing:\n",
        "  * median_income\n",
        "  * housing_median_age\n",
        "  * population\n",
        "- The values aren't missing randomly:\n",
        "  * Missing median_income relates to house values\n",
        "  * Missing housing_age relates to house size\n",
        "  * Missing population relates to both size and value\n",
        "\n",
        "Based on these findings, I decided to  impute all three features\n",
        "- Features should only be removed if more than 30% of values are missing - the 10% is well below this [4]\n",
        "- The earlier analysis showed this is Missing at Random (MAR) data, which means other features can make good predictions [4]\n",
        "- Removing any features would make the analysis less complete [4]\n",
        "\n"
      ],
      "metadata": {
        "id": "8eLezi0Dkjmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_imputation(data, method):\n",
        "    if method == 'knn':\n",
        "        imputer = KNNImputer(n_neighbors=20)\n",
        "    else:  # mice\n",
        "        imputer = IterativeImputer(random_state=42)\n",
        "\n",
        "    imputed_data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "    return imputed_data\n",
        "\n",
        "# Run both imputation methods\n",
        "houses_knn = perform_imputation(houses_corrupted, 'knn')\n",
        "houses_mice = perform_imputation(houses_corrupted, 'mice')\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nImputation Results Comparison:\")\n",
        "\n",
        "for col in missing_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    original = houses[col]\n",
        "    knn_imp = houses_knn[col]\n",
        "    mice_imp = houses_mice[col]\n",
        "\n",
        "    # Distribution statistics\n",
        "    print(\"\\nDistribution Metrics:\")\n",
        "    if col in skewed_cols:\n",
        "        print(\"Center and Spread (Median, MAD):\")\n",
        "        print(f\"Original: {original.median():.3f}, {1.483 * abs(original - original.median()).median():.3f}\")\n",
        "        print(f\"KNN: {knn_imp.median():.3f}, {1.483 * abs(knn_imp - knn_imp.median()).median():.3f}\")\n",
        "        print(f\"MICE: {mice_imp.median():.3f}, {1.483 * abs(mice_imp - mice_imp.median()).median():.3f}\")\n",
        "    else:\n",
        "        print(\"Center and Spread (Mean, Std):\")\n",
        "        print(f\"Original: {original.mean():.3f}, {original.std():.3f}\")\n",
        "        print(f\"KNN: {knn_imp.mean():.3f}, {knn_imp.std():.3f}\")\n",
        "        print(f\"MICE: {mice_imp.mean():.3f}, {mice_imp.std():.3f}\")\n",
        "\n",
        "    print(\"\\nSkewness:\")\n",
        "    print(f\"Original: {original.skew():.3f}\")\n",
        "    print(f\"KNN: {knn_imp.skew():.3f}\")\n",
        "    print(f\"MICE: {mice_imp.skew():.3f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    data = np.concatenate([original, knn_imp, mice_imp])\n",
        "    bin_edges = np.linspace(data.min(), data.max(), 31)\n",
        "    plt.hist(original, bins=bin_edges, alpha=0.5, label='Original', color='blue')\n",
        "    plt.hist(knn_imp, bins=bin_edges, alpha=0.5, label='KNN', color='red')\n",
        "    plt.hist(mice_imp, bins=bin_edges, alpha=0.5, label='MICE', color='yellow')\n",
        "    plt.title(f'{col} Distribution Comparison')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Tu9cYrXpklXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "I tested two imputation methods for the missing values:\n",
        "- KNN imputation using 20 nearest neighbors[9]\n",
        "- MICE (Multiple Imputation by Chained Equations) [9], [10]\n",
        "\n",
        "Compared the center, spread, and skew before and after imputation for each method\n",
        "\n",
        "\n",
        "**Reasoning:**\n",
        "* Compared results to evaluate how well each method preserved core data characteristics [1]\n",
        "* Used different neighbors value because it achieved better performance\n",
        "\n",
        "**Results Analysis:**\n",
        "\n",
        "- For median_income (Right-skewed distribution):\n",
        "  * Center: MICE (3.504) deviates less from the original (3.535) than KNN (3.486)\n",
        "  * Spread: MICE (1.499) deviates less from the original (1.578) than KNN (1.429)\n",
        "  * Skewness: MICE (1.692) deviates less from the original (1.647) than KNN (1.733).\n",
        "  * Visualization: KNN slightly overestimates the peak frequency\n",
        "- For housing_median_age (normal distribution):\n",
        "  * Center: MICE (28.481) deviates less from the original (28.639) than KNN (28.636)\n",
        "  * Spread: KNN (12.036) deviates less from the original (12.586) than MICE (12.025).\n",
        "  * Skewness: MICE (0.052) deviates less from the original (0.060) than KNN (0.019).\n",
        "  * Visualization: KNN and MICE overly concentrate imputations at the center\n",
        "- For population (Highly right-skewed distribution):\n",
        "  * Center: MICE (1152.000) deviates less from the original (1166.000) than KNN (1149.000).\n",
        "  * Spread: KNN (585.785) deviates less from the original (652.520) than MICE (615.445).\n",
        "  * Skewness: MICE (4.971) deviates less from the original (4.936) than KNN (5.026).\n",
        "  * Visualization: Both methods almost exactly mirror the distribution\n",
        "\n",
        "Based on these findings, I recommend using MICE imputation\n",
        "- The best method more closely preserves data characteristics [4]\n",
        "- MICE was consistently better at maintaining the center and skewness of the distributions\n",
        "- While KNN was sometimes better with spread, MICE performed better on average"
      ],
      "metadata": {
        "id": "OxCCo3RrknFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each dataset, use all features except target for regression\n",
        "target = 'median_house_value'\n",
        "features = [col for col in houses.columns if col != target]\n",
        "\n",
        "# Compare regression performance across datasets\n",
        "datasets = {\n",
        "    'Original': houses,\n",
        "    'KNN Imputed': houses_knn,\n",
        "    'MICE Imputed': houses_mice\n",
        "}\n",
        "\n",
        "print(\"Regression Results:\\n\")\n",
        "for name, data in datasets.items():\n",
        "    # Prepare data - using multiple predictors and polynomial features\n",
        "    X = data[features].values\n",
        "    y = data[target].values\n",
        "\n",
        "    # Split into 80% training, 20% testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=3, include_bias=False)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "\n",
        "    # Fit model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = root_mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n{name} Dataset:\")\n",
        "    print(f\"RÂ² Score: {r2:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "\n",
        "    # Visualize predicted vs actual values\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual House Value')\n",
        "    plt.ylabel('Predicted House Value')\n",
        "    plt.title(f'{name} Dataset: Predicted vs Actual Values')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "e3tA2KOrkqCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "- Built regression models to predict house values\n",
        "- Used the same features for each dataset (original, KNN, and MICE)\n",
        "- Created polynomial features to catch non-linear relationships\n",
        "- Compared model performance using RÂ² and RMSE\n",
        "\n",
        "\n",
        "**Reasoning:**\n",
        "* Used regression analysis to test of imputation quality\n",
        "* Added polynomial features to consider non-linear patterns in the distribution [6], [12]\n",
        "* Compared both RÂ² and RMSE to evaluate model performance across different value ranges [3]\n",
        "\n",
        "**Results Analysis**\n",
        "- Original data:\n",
        "  * RÂ² = 0.6565, RMSE = 67096.22\n",
        "  * Sets baseline performance\n",
        "- KNN imputed data:\n",
        "  * RÂ² = 0.6390, RMSE = 68779.92\n",
        "  * Best imputation performance overall\n",
        "- MICE imputed data:\n",
        "  * RÂ² = 0.6374, RMSE = 68930.63\n",
        "  * Slight decline from the original\n",
        "- All models:\n",
        "  * Predicted well for average houses\n",
        "  * Had trouble with very expensive houses\n",
        "\n",
        "Based on these findings I continue to recommend using MICE imputation\n",
        "- While KNN performed regression better, the difference was negligible and MICE also preserved distributions better\n",
        "- The similar prediction plots show both methods kept the important relationships between features\n"
      ],
      "metadata": {
        "id": "jgJCtPkOkrid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Conclusions & Throughts [3]\n",
        "- Disucss methods used for anomaly detection, pros/cons of each method\n",
        "- Disucss challenges/difficulties in anomaly detection implementation\n",
        "- Discuss methods used for imputation, pros/cons of each method\n",
        "- Discuss challenges/difficulties in imputation implementation"
      ],
      "metadata": {
        "id": "lb2mlORvkt20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My analysis of the housing dataset revealed important discoveries about outlier detection and data imputation. I found statistical methods offered significant advantages in outlier detection despite their simplicity. The robust z-score method was particularly effective. Its use of median and MAD made it especially appropriate for my skewed housing data [1]. However, it did show some weakness with my bimodal geographic features.\n",
        "\n",
        "While algorithmic methods like LOF, SVM, and Isolation Forest could theoretically capture more complex patterns, they presented several practical challenges. LOF struggled with high-density regions, SVM showed problematic strip patterns, and Isolation Forest was generally oversensitive to outliers. These methods also required careful parameter tuning and significant computational resources, making them less practical for my particular dataset [4].\n",
        "\n",
        "For imputation, both KNN and MICE showed distinct strengths and weaknesses. KNN imputation proved more intuitive to implement and better performed regression. However, it struggled to maintain the center and skew of my features. MICE was more complex to implement and computationally intensive. However, it better preserved the center and skew of my distributions and performed regression almost as well as KNN. Its main drawback was poorer preservation of spread values.\n",
        "\n",
        "Several challenges emerged during implementation. For outlier detection, I struggled with handling different distribution types among my features. Imputation challenged me with feature selection and parameter tuning, particularly in balancing computational efficiency with accuracy. The visualization and comparison of results across multiple methods also proved difficult."
      ],
      "metadata": {
        "id": "8bN7M5gYkvk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Citations\n",
        "\n",
        "[1] I. Dixon. (2024). Lecture 1b: Exploratory Data Analysis [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/pages/lecture?module_item_id=3467277\n",
        "\n",
        "[2] I. Dixon. (2024). Practical I [Google Colab notebook]. Available: https://github.com/iaindixon/CSC3831_Part_I/blob/main/CSC3831_Partical_I.ipynb\n",
        "\n",
        "[3] P. Missier. (201). Beer Imputation Example [Google Colab notebook]. Available: https://github.com/PaoloMissier/CSC3831-2021-22/blob/main/IMPUTATION/regression-example/Beer-imputation-example.ipynb\n",
        "\n",
        "[4] I. Dixon. (2024). Lecture 2b: Data Imputation [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/pages/lecture-2?module_item_id=3503618\n",
        "\n",
        "[5] I. Dixon. (2024). Lecture 3a: Tabular Anomaly Detection [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/pages/lecture-3?module_item_id=3511620\n",
        "\n",
        "[6] https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\n",
        "\n",
        "[7] Scikit Learn. \"SVM.\" scikit-learn.org. Accessed: Dec. 3, 2024. [Online.] Available: https://scikit-learn.org/stable/modules/svm.html\n",
        "\n",
        "\n",
        "[8] Scikit Learn. \"Neighbors.\" scikit-learn.org. Accessed: Dec. 3, 2024. [Online.] Available: https://scikit-learn.org/stable/modules/neighbors.html\n",
        "\n",
        "[9] Scikit Learn. \"Impute.\" scikit-learn.org. Accessed: Dec. 4, 2024. [Online.] Available: https://scikit-learn.org/stable/api/sklearn.impute.html\n",
        "\n",
        "[10] Azur, M.J., Stuart, E.A., Frangakis, C. and Leaf, P.J., \"Multiple imputation by chained equations: what is it and how does it work?.\" Int. J. Methods Psychiatr. Res., 20: 40-49. 2011. [Online.] Available: https://doi.org/10.1002/mpr.329\n",
        "\n",
        "[11] V. Gonzalez-Zelaya. (2024). Introduction to Machine Learning / Regression Problems [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/files/8847813?module_item_id=3517175\n",
        "\n",
        "[12] Scikit Learn. \"Preprocessing.\" scikit-learn.org. Accessed: Dec. 4, 2024. [Online.] Available: https://scikit-learn.org/stable/modules/preprocessing.html"
      ],
      "metadata": {
        "id": "-nXHClEmkx0u"
      }
    }
  ]
}