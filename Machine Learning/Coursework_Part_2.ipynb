{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpBkMMy9Yt4htdMrNFbTJH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1BLbVAstk7hq"},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","source":["# Load the MNIST dataset\n","X, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto', as_frame=False)\n","\n","# Create a plot with 2 rows and 10 columns for 20 digits\n","fig, axes = plt.subplots(2, 10, figsize=(12, 3))\n","axes = axes.ravel()\n","\n","# Get and reshape first 20 examples from 784 pixels to 28x28 images\n","X_subset = X[:20].reshape(-1, 28, 28)\n","y_subset = y[:20]\n","\n","# Plot each digit\n","for idx, (ax, img, label) in enumerate(zip(axes, X_subset, y_subset)):\n","    ax.imshow(img, cmap='binary')\n","    ax.set_title(label)\n","    ax.axis('off')  # Hide axes\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"touDAeH9lH1w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Used fetch_openml to get the MNIST handwritten digit dataset [1], [2]\n","* Created a visualization grid showing 20 sample digits\n","* Added labels and removed axes for cleaner visualization\n","\n","**Reasoning:**\n","* Removed axis ticks since they don't provide meaningful information for images [2]\n","* Used binary colormap since handwritten digits are normally black and white [2]\n","\n","**Results Analysis:**\n","* The dataset holds 784 features (28x28 pixels)\n","* There are 10 classes representing digits 0-9\n","* The images show clear digits with variation in writing styles"],"metadata":{"id":"VvSxwa-_lKjO"}},{"cell_type":"markdown","source":["### 2. Train a Logistic Regression classifier on this data, and report on your findings.\n","    \n","1. Tune your hyperparameters to ensure *sparse* weight vectors and high accuracy.\n","2. Visualise the classification vector for each class."],"metadata":{"id":"Xl1giJoxlMze"}},{"cell_type":"code","source":["# Split data into training and test sets (80-20 split)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train logistic regression\n","log_reg = LogisticRegression(\n","    penalty='l1',     # Use L1 regularization for sparse weights\n","    solver='saga',    # Solver that works well with L1\n","    tol=0.001,        # Allows for easier convergance\n","    C=0.05,           # Strength of regularization\n","    random_state=42,  # For reproducability\n","    max_iter=200,     # Maximum number of iterations\n","    n_jobs=-1         # Use all available CPU cores\n",")\n","\n","# Fit model and make predictions\n","log_reg.fit(X_train_scaled, y_train)\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Print model performance\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Calculate sparsity (percentage of zero coefficients)\n","sparsity = np.mean(log_reg.coef_ == 0) * 100\n","print(f'\\nSparsity: {sparsity:.1f}% of coefficients are zero')"],"metadata":{"id":"VuQL6gpglPQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Split data using train_test_split with:\n","  * 80-20 train-test ratio\n","  * random_state=42 for reproducibility\n","* Standardized features using StandardScaler\n","* Configured LogisticRegression with:\n","  * penalty='l1'\n","  * solver='saga'\n","  * C=0.05\n","  * tol=0.001\n","  * random_state = 42\n","  * max_iter=200\n","  * n_jobs=-1\n","* Printed accuracy and classification report\n","\n","**Reasoning:**\n","* test_size=0.2\n","  * Ensures enough training data [3]\n","  * Standard split ratio for large datasets [3]\n","* penalty='l1' [1]. [4], [5]\n","  * Creates sparse weight vectors\n","* solver='saga' [1], [4], [5], [6]\n","  * Optimized for L1 regularization\n","  * Works well with many features\n","* C=0.05 [1], [4], [5], [6]\n","  * Strong regularization for sparsity\n","  * Found through trial and error\n","* tol=0.001 [1], [4], [5], [6]\n","  * Strict enough for convergence\n","  * Not too strict to slow training\n","* random_state=42 [4], [7]\n","  * Ensures reproducability [4]\n","* max_iter=200 [4], [6]\n","  * Allows model to converge\n","  * Despite warning, achieved good results\n","* n_jobs=-1 [4]\n","  * Uses all CPU cores\n","  * Speeds up training\n","  * No downside on personal machine\n","\n","**Results Analysis:**\n","* Reached 91.79% on test set\n","* Consistently high performance across digits\n","* Some digits harder to classify than others\n","  * Best: Digit 0 (96% precision)\n","  * Worst: Digit 5 (88% precision)\n","  * Most balanced: Digit 6 (94% precision, 95% recall)\n","* Sparsity\n","  * 51.8% of coefficients are zero\n","  * Shows effective feature selection\n","  * Keeps high accuracy despite simplification"],"metadata":{"id":"Ytow7syylQyA"}},{"cell_type":"code","source":["# Visualize classification vectors for each digit\n","fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","axes = axes.ravel()\n","\n","for digit in range(10):\n","    # Get coefficients for current digit and reshape to image dimensions\n","    coef = log_reg.coef_[digit].reshape(28, 28)\n","    axes[digit].imshow(coef, cmap='RdBu')\n","    axes[digit].axis('off')\n","    axes[digit].set_title(f'Digit {digit}')\n","\n","plt.show()"],"metadata":{"id":"uIBMwbFQlTKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Created a 2x5 grid of subplots\n","* Reshaped classification vectors to 28x28 images\n","* Displayed using RdBlu colormap\n","\n","**Reasoning:**\n","* Selected RdBu colormap [1]\n","  * Red shows positive weights (features supporting classification)\n","  * Blue shows negative weights (features opposing classification)\n","  * White indicates neutral or zero weights\n","* Removed axes [1]\n","  * Pixel coordinates aren't meaningful\n","  * Makes visualization cleaner\n","  * Matches style of original digit display\n","\n","**Results Analysis:**\n","* Vague patterns are interpretable\n","  * Digit 0 shows circular pattern\n","  * Digit 8 shows overlapping circles\n","* Sparsity is visible\n","  * Large white areas in some visualizations\n","  * Sharp transitions between important/unimportant regions\n"],"metadata":{"id":"ia2RxooIlUmZ"}},{"cell_type":"markdown","source":["### 3. Use PCA to reduce the dimensionality of your training data.\n","    \n","1. Determine the number of components necessary to explain 80\\% of the variance\n","2. Plot the explained variance by number of components.\n","3. Visualise the 20 principal components' loadings\n","4. Plot the two principal components for your data using a scatterplot, colouring by class. What can you say about this plot?\n","5. Visualise the first 20 digits, *generated from their lower-dimensional representation*."],"metadata":{"id":"ySDjkx6xlW0i"}},{"cell_type":"code","source":["# Scale the data for PCA\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Find number of components for 80% variance\n","pca = PCA()\n","pca.fit(X_scaled)\n","cumsum = np.cumsum(pca.explained_variance_ratio_)\n","n_components_80 = np.argmax(cumsum >= 0.8) + 1\n","print(f\"Components needed for 80% variance: {n_components_80}\")"],"metadata":{"id":"CejZKJbPlYp8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Scaled features using StandardScaler\n","* Applied PCA without dimension reduction\n","* Computed cumulative explained variance\n","* Found minimum components for 80% variance threshold\n","\n","**Reasoning:**\n","* Applied StandardScaler\n","  * PCA is sensitive to feature scales [8]\n","* Used full PCA initially to ensure proper component selection [9], [10]\n","* Calculated cumulative variance to choose optimal components [9]\n","\n","**Results Analysis:**\n","* Components needed: 150\n","* 784 dimensions originally\n","* ~81% reduction in features"],"metadata":{"id":"MY_EcvwBlaJD"}},{"cell_type":"code","source":["# Plot explained variance\n","plt.figure(figsize=(10, 4))\n","plt.plot(cumsum)\n","plt.axhline(y=0.8, color='r', linestyle='--')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('PCA Explained Variance')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"5B26F9vUlcZp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Plotted cumulative variance against number of components\n","* Added a horizontal line at 80% threshold\n","\n","**Reasoning:**\n","* Used line plot because it shows continuous build up of variance [9]\n","* Added red dashed line at 0.8 to highlight the target threshold\n","\n","**Results Analysis:**\n","* Sharp initial rise (first ~50 components)\n","  * First few components capture most variance\n","* Gradual flattening after elbow\n","  * Diminishing returns after ~150 components\n","* 80% threshold intersection occurs at 150 components\n"],"metadata":{"id":"5MlYsDmAldsv"}},{"cell_type":"code","source":["# Visualize first 20 principal components\n","fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n","axes = axes.ravel()\n","\n","for idx in range(20):\n","    component = pca.components_[idx].reshape(28, 28)\n","    axes[idx].imshow(component, cmap='RdBu')\n","    axes[idx].axis('off')\n","    axes[idx].set_title(f'PC {idx+1}')\n","\n","plt.show()"],"metadata":{"id":"Rv-z-HFylfPB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Created a 4x5 grid of subplots\n","* Reshaped components to 28x28 images\n","* Displayed using RdBu colormap\n","\n","**Reasoning:**\n","* Selected RdBu colormap [2], [10], [11]\n","  * Red shows positive contributions\n","  * Blue shows negative contributions\n","  * White indicates neutral areas\n","  * Matches the logistic regression visualizations\n","* Removed axes\n","  * Makes visualization cleaner [2]\n","  * Focuses attention on patterns\n","\n","**Results Analysis:**\n","* First components show basic digit structures and large-scale patterns\n","* Later components capture more complex patterns\n","\n"],"metadata":{"id":"hhzR9ehAlgsI"}},{"cell_type":"code","source":["# Show data in 2D using first two components\n","pca_2d = PCA(n_components=2)\n","X_pca = pca_2d.fit_transform(X_scaled)\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(X_pca[:, 0], X_pca[:, 1],\n","           c=y.astype(int),\n","           cmap='tab10',\n","           alpha=0.5)\n","plt.xlabel('First Principal Component')\n","plt.ylabel('Second Principal Component')\n","plt.colorbar(label='Digit')\n","plt.show()"],"metadata":{"id":"rgg4eNAElibV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Reduced data to 2 dimensions using PCA [10]\n","* Created a scatter plot of all samples [11]\n","* Colored points by digit class\n","* Added axis labels and colorbar\n","\n","**Reasoning:**\n","* Selected tab10 colormap\n","  * Designed for categorical data\n","  * Each digit gets distinct color\n","* alpha=0.5\n","  * Show point density\n","  * Display overlapping regions\n","\n","**Results Analysis:**\n","  * Imperfect separation\n","    * Makes sense given high dimension reduction\n","  * Complex digits look clustered (3, 4, 5, 8)\n","  * Distinct digits are spread further apart (7, 1, 0)"],"metadata":{"id":"CrdSIW1BljrG"}},{"cell_type":"code","source":["# Show reconstructed digits using reduced dimensions\n","pca = PCA(n_components=n_components_80)\n","X_reduced = pca.fit_transform(X_scaled)\n","X_reconstructed = pca.inverse_transform(X_reduced)\n","X_reconstructed = scaler.inverse_transform(X_reconstructed)\n","\n","# Plot first 20 reconstructed digits\n","fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n","axes = axes.ravel()\n","\n","for i in range(20):\n","    axes[i].imshow(X_reconstructed[i].reshape(28, 28), cmap='binary')\n","    axes[i].axis('off')\n","    axes[i].set_title(f'Digit {y[i]}')\n","\n","plt.show()"],"metadata":{"id":"ORExKTu_llPF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Reduced data to 80% variance components [9], [10], [12]\n","* Reconstructed digits from reduced space [10]\n","* Transformed back to original scale\n","* Displayed first 20 reconstructed digits [2]\n","\n","**Reasoning:**\n","* Used n_components_80 to match the earlier variance analysis\n","* Chose binary colormap because it matches original MNIST style\n","\n","**Results Analysis:**\n","  * Main digit shapes preserved\n","  * Slight blurring in complex regions\n","  * Shows 80% variance is sufficient to perserve information"],"metadata":{"id":"yoeyIKy3lm4w"}},{"cell_type":"markdown","source":["### 4. Generate a noisy copy of your data by adding random normal noise to the digits **with a scale that doesn't completely destroy the signal**. This is, the resulting images noise should be apparent, but the numbers should still be understandable.\n","    \n","1. Visualise the first 20 digits from the noisy dataset.\n","2. Filter the noise by fitting a PCA explaining **a sufficient proportion** of the variance, and then transforming the noisy dataset. Figuring out this proportion is part of the challenge.\n","3. Visualise the first 20 digits of the de-noised dataset."],"metadata":{"id":"EE023rnXlqLL"}},{"cell_type":"code","source":["# Add random noise to images\n","noise_level = 60\n","X_noisy = X + np.random.normal(0, noise_level, X.shape)\n","\n","# Show noisy digits\n","fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n","axes = axes.ravel()\n","\n","for i in range(20):\n","    axes[i].imshow(X_noisy[i].reshape(28, 28), cmap='binary')\n","    axes[i].axis('off')\n","    axes[i].set_title(f'Digit {y[i]}')\n","\n","plt.show()"],"metadata":{"id":"hTBBFNbZlrtq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Added noise with scale=60 [13]\n","* Maintained original image dimensions\n","* Visualized first 20 noisy digits [2]\n","\n","**Reasoning:**\n","* noise_level=60\n","  * Make noise clearly visible\n","  * Keep digits recognizable\n","  * Create challenging denoising task\n","* Used binary colormap to match original MNIST style\n","\n","**Results Analysis:**\n","  * Clear random variations added\n","  * Digit shapes still visible\n","  * Varying impact on different images"],"metadata":{"id":"VlTIIImJltAJ"}},{"cell_type":"code","source":["# Scale and denoise using PCA\n","scaler = StandardScaler()\n","X_noisy_scaled = scaler.fit_transform(X_noisy)\n","\n","# PCA with 90% variance kept\n","pca = PCA(n_components=0.90)\n","X_denoised = pca.inverse_transform(pca.fit_transform(X_noisy_scaled))\n","X_denoised = scaler.inverse_transform(X_denoised)\n","\n","print(f\"Using {pca.n_components_} components for denoising\")"],"metadata":{"id":"tHI69e7VlvBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Standardized the noisy images\n","* Applyied PCA with 90% variance threshold [9], [10]\n","* Reconstructed denoised images [10]\n","\n","**Reasoning:**\n","* 90% variance threshold [9]\n","  * Higher than 80% used for compression\n","  * Better separates noise from signal\n","\n","**Results Analysis:**\n","  * Higher than 150 components used for compression\n","  * Appropriate due to noise presence"],"metadata":{"id":"XkO1MZ7Olwhk"}},{"cell_type":"code","source":["# Show denoised digits\n","fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n","axes = axes.ravel()\n","\n","for i in range(20):\n","    axes[i].imshow(X_denoised[i].reshape(28, 28), cmap='binary')\n","    axes[i].axis('off')\n","    axes[i].set_title(f'Digit {y[i]}')\n","\n","plt.show()"],"metadata":{"id":"7tcy5Jt-lyeK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary:**\n","* Created 4x5 grid of denoised digits\n","* Reshaped each to 28x28 images\n","* Used binary colormap display\n","\n","**Reasoning:**\n","* Kept binary colormap because it matches original MNIST style\n","* Removed axes [2]\n","  * Keep visualization clean\n","  * Focus on denoising quality\n","  * Maintain consistent style\n","\n","**Results Analysis:**\n","  * Significant noise removal\n","  * Cleaner digit boundaries\n","  * Some loss compared to originals"],"metadata":{"id":"q29jWN8olzmV"}},{"cell_type":"markdown","source":["#### Citations\n","[1] Scikit Learn. \"MNIST classification using multinomial logistic + L1.\" scikit-learn.org. Accessed: Dec. 1, 2024. [Online.] Available: https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html\n","\n","[2] V. Ojha. (2024). Image Classification DNNs [Google Colab notebooks]. Available: https://ncl.instructure.com/courses/55046/modules/items/3539140\n","\n","[3] V. Gonzalez-Zelaya. (2024). Introduction to Machine Learning / Regression Problems [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/files/8847813?module_item_id=3517175\n","\n","[4] Scikit Learn. \"LogisticRegression.\" scikit-learn.org. Accessed: Dec. 2, 2024. [Online.] Available: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","\n","\n","[5] V. Ojha. (2024). Convolutional Neural Network [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/files/8928967?module_item_id=3535263\n","\n","[6] A. Mani. \"MNIST digits classification using Logistic regression in Scikit-Learn.\" atmamani.github.io. Accessed: Dec. 1, 2024. [Online.] Available: https://atmamani.github.io/projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/\n","\n","[7] V. Gonzalez-Zelaya. (2024). Practical 2 [Google Colab notebook]. Available: https://colab.research.google.com/drive/1_8ODB5_nC0p9mh9wl3wGY3xxD5L_rLV5?usp=sharing\n","\n","[8] V. Gonzalez-Zelaya. (2024). Unsupervised Learning [PowerPoint slides]. Available: https://ncl.instructure.com/courses/55046/files/8884895?module_item_id=3528211\n","\n","[9] V. Gonzalez-Zelaya. (2024). Principal Components Analysis (PCA) [Google Colab notebook]. Available: https://colab.research.google.com/drive/1rwHmzIA18PTudDy_HzIDQ0HKbkmk_wbZ?usp=sharing\n","\n","\n","[10] Scikit Learn. \"PCA.\" scikit-learn.org. Accessed: Dec. 2, 2024. [Online.] Available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n","\n","[11] A. Tam. \"Principal Component Analysis for Visualization.\" machinelearningmastery.com. Accessed: Nov. 28, 2024. [Online.] Available: https://machinelearningmastery.com/principal-component-analysis-for-visualization/\n","\n","\n","[12] Stack Overflow. \"PCA projection and reconstruction in scikit-learn.\" stackoverflow.com. Accessed: Dec. 3, 2024. [Online.] Available: https://stackoverflow.com/questions/36566844/pca-projection-and-reconstruction-in-scikit-learn\n","\n","[13] M. Somanna. \"Guide to Adding Noise to Synthetic Data using Python and Numpy.\" medium.com. Accessed: Dec. 1, 2024. [Online.] Available: https://medium.com/@ms_somanna/guide-to-adding-noise-to-your-data-using-python-and-numpy-c8be815df524"],"metadata":{"id":"evvUKh_Rl2on"}}]}